{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "En este ejercicio, vamos a implementar un árbol de decisión desde 0, empleandolo para clasificar setas en venenosas o no venenosas, en función de sus características.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Importar paquetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 -  Descripción del problema\n",
    "\n",
    "Supongamos que está creando una empresa que cultiva y vende setas silvestres. \n",
    "- Necesitamos saber si una determinada seta es comestible o venenosa basándose en sus atributos físicos.\n",
    "\n",
    "\n",
    "## 3 - Dataset\n",
    "\n",
    "Cargamos el dataset, que tiene el siguiente aspecto\n",
    "\n",
    "|                                                     | Cap Color | Stalk Shape | Solitary | Edible |\n",
    "|:---------------------------------------------------:|:---------:|:-----------:|:--------:|:------:|\n",
    "| <img src=\"images/0.png\" alt=\"drawing\" width=\"50\"/> |   Brown   |   Tapering  |    Yes   |    1   |\n",
    "| <img src=\"images/1.png\" alt=\"drawing\" width=\"50\"/> |   Brown   |  Enlarging  |    Yes   |    1   |\n",
    "| <img src=\"images/2.png\" alt=\"drawing\" width=\"50\"/> |   Brown   |  Enlarging  |    No    |    0   |\n",
    "| <img src=\"images/3.png\" alt=\"drawing\" width=\"50\"/> |   Brown   |  Enlarging  |    No    |    0   |\n",
    "| <img src=\"images/4.png\" alt=\"drawing\" width=\"50\"/> |   Brown   |   Tapering  |    Yes   |    1   |\n",
    "| <img src=\"images/5.png\" alt=\"drawing\" width=\"50\"/> |    Red    |   Tapering  |    Yes   |    0   |\n",
    "| <img src=\"images/6.png\" alt=\"drawing\" width=\"50\"/> |    Red    |  Enlarging  |    No    |    0   |\n",
    "| <img src=\"images/7.png\" alt=\"drawing\" width=\"50\"/> |   Brown   |  Enlarging  |    Yes   |    1   |\n",
    "| <img src=\"images/8.png\" alt=\"drawing\" width=\"50\"/> |    Red    |   Tapering  |    No    |    1   |\n",
    "| <img src=\"images/9.png\" alt=\"drawing\" width=\"50\"/> |   Brown   |  Enlarging  |    No    |    0   |\n",
    "\n",
    "\n",
    "- Disponemos de 10 especies. Cada seta es un ejemplo para el que tenemos:\n",
    "    - Tres características (features): Xs\n",
    "        - Color del \"sombrero\" (`Marron` or `Red`),\n",
    "        - Forma del pie (`Tapering (as in \\/)` or `Enlarging (as in /\\)`), and\n",
    "        - Solitaria (`Yes` or `No`)\n",
    "    - Etiqueta: y\n",
    "        - Comestible (`1` indicando si `0` indicando no (es venenosa))\n",
    "\n",
    "\n",
    "### 3.1 One hot encoding\n",
    "Convertimos los valores en 1s o 0s para que el algoritmo pueda trabajar con ellos.\n",
    "\n",
    "|                                                    | Brown Cap | Tapering Stalk Shape | Solitary | Edible |\n",
    "|:--------------------------------------------------:|:---------:|:--------------------:|:--------:|:------:|\n",
    "| <img src=\"images/0.png\" alt=\"drawing\" width=\"50\"/> |     1     |           1          |     1    |    1   |\n",
    "| <img src=\"images/1.png\" alt=\"drawing\" width=\"50\"/> |     1     |           0          |     1    |    1   |\n",
    "| <img src=\"images/2.png\" alt=\"drawing\" width=\"50\"/> |     1     |           0          |     0    |    0   |\n",
    "| <img src=\"images/3.png\" alt=\"drawing\" width=\"50\"/> |     1     |           0          |     0    |    0   |\n",
    "| <img src=\"images/4.png\" alt=\"drawing\" width=\"50\"/> |     1     |           1          |     1    |    1   |\n",
    "| <img src=\"images/5.png\" alt=\"drawing\" width=\"50\"/> |     0     |           1          |     1    |    0   |\n",
    "| <img src=\"images/6.png\" alt=\"drawing\" width=\"50\"/> |     0     |           0          |     0    |    0   |\n",
    "| <img src=\"images/7.png\" alt=\"drawing\" width=\"50\"/> |     1     |           0          |     1    |    1   |\n",
    "| <img src=\"images/8.png\" alt=\"drawing\" width=\"50\"/> |     0     |           1          |     0    |    1   |\n",
    "| <img src=\"images/9.png\" alt=\"drawing\" width=\"50\"/> |     1     |           0          |     0    |    0   |\n",
    "\n",
    "\n",
    "- `X_train` contiene las características de cada ejemplo (especie de seta) \n",
    "    - Color marrón (`1` indica \"Marrón\" y `0` indica \"Rojo\")\n",
    "    - Forma del pie (`1` indica \"Forma de pie cónico\" y `0` indica \"Forma de pie ensanchado\")\n",
    "    - Solitaria  (`1` indica \"Sí\" y `0` indica \"No\")\n",
    "\n",
    "\n",
    "- `y_train` indica si la seta es comestible o no\n",
    "    - `y = 1` indica comestible\n",
    "    - `y = 0` indica venenosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([[1,1,1],[1,0,1],[1,0,0],[1,0,0],[1,1,1],[0,1,1],[0,0,0],[1,0,1],[0,1,0],[1,0,0]])\n",
    "y_train = np.array([1,1,0,0,1,0,0,1,1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeros elementos de X_train:\n",
      " [[1 1 1]\n",
      " [1 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 1 1]]\n",
      "Tipo de X_train: <class 'numpy.ndarray'>\n",
      "Forma de X_train is: (10, 3)\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Primeros elementos de y_train: [1 1 0 0 1]\n",
      "Tipo de y_train: <class 'numpy.ndarray'>\n",
      "Forma de y_train: (10,)\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Número de ejemplos de entrenamiento (m): 10\n"
     ]
    }
   ],
   "source": [
    "print(\"Primeros elementos de X_train:\\n\", X_train[:5])\n",
    "print(\"Tipo de X_train:\",type(X_train))\n",
    "print('Forma de X_train is:', X_train.shape)\n",
    "\n",
    "print(\"\\n\" + \"-\"*100 + \"\\n\")\n",
    "\n",
    "print(\"Primeros elementos de y_train:\", y_train[:5])\n",
    "print(\"Tipo de y_train:\",type(y_train))\n",
    "print(\"Forma de y_train:\", y_train.shape)\n",
    "\n",
    "print(\"\\n\" + \"-\"*100 + \"\\n\")\n",
    "\n",
    "print('Número de ejemplos de entrenamiento (m):', len(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Decision Tree\n",
    "\n",
    "\n",
    "Los pasos para construir un árbol de decisión son los siguientes:\n",
    "- Comenzar con todos los ejemplos en el nodo raíz\n",
    "- Calcular la ganancia de información para dividir en todas las características posibles y elegir la que tenga la mayor ganancia de información\n",
    "- Dividir el conjunto de datos según la característica seleccionada y crear ramas izquierda y derecha del árbol\n",
    "- Repetir el proceso de división hasta que se cumpla el criterio de parada\n",
    "\n",
    "Vamos a prepararlas siguientes funciones para construir el árbol de decisión:\n",
    "- Calcular la entropía en un nodo\n",
    "- Dividir el conjunto de datos en un nodo en ramas izquierda y derecha basadas en una característica dada\n",
    "- Calcular la ganancia de información de dividir en una característica dada\n",
    "- Elegir la característica que maximice la ganancia de información\n",
    "\n",
    "Repetiremos el proceso de división hasta que se cumpla el criterio de parada. \n",
    "En este ejercicio, el criterio de parada es establecer una profundidad máxima de 2.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1  Calcular entropía\n",
    "\n",
    "Preparamos una función para calcular la entrepía en un nodo, esto es, la impureza de los datos en el nodo.\n",
    "\n",
    "La función toma un array (`y`) que indica si los ejemplos en ese nodo son comestibles (`1`) o venenosos (`0`).\n",
    "El resultado es un valor que indica la impureza (proporción de ejemplos venenosos) en ese nodo.\n",
    "\n",
    "$$H(p_1) = -p_1 \\text{log}_2(p_1) - (1- p_1) \\text{log}_2(1- p_1)$$\n",
    "\n",
    "* Notas:\n",
    "    * El logaritmo se calcula con base $2$\n",
    "    * Para fines de implementación, $0\\text{log}_2(0) = 0$. Es decir, si `p_1 = 0` o `p_1 = 1`, la entropía es `0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_entropy(y):\n",
    "    \"\"\"\n",
    "    Computes the entropy for \n",
    "    \n",
    "    Args:\n",
    "       y (ndarray): Numpy array indicating whether each example at a node is\n",
    "           edible (`1`) or poisonous (`0`)\n",
    "       \n",
    "    Returns:\n",
    "        entropy (float): Entropy at that node\n",
    "        \n",
    "    \"\"\"\n",
    "    entropy = 0.\n",
    "    \n",
    "    if len(y) != 0:   \n",
    "        p1 = len(y[y==1]) / len(y)\n",
    "        p0 = 1-p1\n",
    "        \n",
    "        if p1 != 0 and p1 != 1:\n",
    "            entropy = -p1*np.log2(p1)-p0*np.log2(p0)   \n",
    "    \n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecutamos la función con todos los datos, root node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropía root node:  1.0\n"
     ]
    }
   ],
   "source": [
    "# En los datos de entrada tenemos 5 setas comestibles y 5 venenosas, por lo que la entropía es 1.\n",
    "\n",
    "print(\"Entropía root node: \", compute_entropy(y_train)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Dividir los datos por ramas izquierda y derecha\n",
    "\n",
    "Escribiremos una función auxiliar llamada `split_dataset` que toma los datos en un nodo y una característica para dividir y los divide en ramas izquierda y derecha:\n",
    "- La función toma los datos en un nodo, `X`, y una característica para dividir, `feature`.\n",
    "- La función devuelve los índices de los datos en `X` que van a la rama izquierda y los índices de los datos en `X` que van a la rama derecha.\n",
    "- Por ejemplo, en el nodo raíz, `node_indices = [0,1,2,3,4,5,6,7,8,9]`, y elegimos dividir en la característica `0`, que es si la seta tiene un sombrero marrón.\n",
    "    - La salida de la función es entonces, `left_indices = [0,1,2,3,4,7,9]` (puntos de datos con sombrero marrón) y `right_indices = [5,6,8]` (puntos de datos sin sombrero marrón)\n",
    "    \n",
    "    \n",
    "|       |                                                    | Brown Cap | Tapering Stalk Shape | Solitary | Edible |\n",
    "|-------|:--------------------------------------------------:|:---------:|:--------------------:|:--------:|:------:|\n",
    "| 0     | <img src=\"images/0.png\" alt=\"drawing\" width=\"50\"/> |     1     |           1          |     1    |    1   |\n",
    "| 1     | <img src=\"images/1.png\" alt=\"drawing\" width=\"50\"/> |     1     |           0          |     1    |    1   |\n",
    "| 2     | <img src=\"images/2.png\" alt=\"drawing\" width=\"50\"/> |     1     |           0          |     0    |    0   |\n",
    "| 3     | <img src=\"images/3.png\" alt=\"drawing\" width=\"50\"/> |     1     |           0          |     0    |    0   |\n",
    "| 4     | <img src=\"images/4.png\" alt=\"drawing\" width=\"50\"/> |     1     |           1          |     1    |    1   |\n",
    "| 5     | <img src=\"images/5.png\" alt=\"drawing\" width=\"50\"/> |     0     |           1          |     1    |    0   |\n",
    "| 6     | <img src=\"images/6.png\" alt=\"drawing\" width=\"50\"/> |     0     |           0          |     0    |    0   |\n",
    "| 7     | <img src=\"images/7.png\" alt=\"drawing\" width=\"50\"/> |     1     |           0          |     1    |    1   |\n",
    "| 8     | <img src=\"images/8.png\" alt=\"drawing\" width=\"50\"/> |     0     |           1          |     0    |    1   |\n",
    "| 9     | <img src=\"images/9.png\" alt=\"drawing\" width=\"50\"/> |     1     |           0          |     0    |    0   |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(X, node_indices, feature):\n",
    "    \"\"\"\n",
    "    Splits the data at the given node into\n",
    "    left and right branches\n",
    "    \n",
    "    Args:\n",
    "        X (ndarray):             Data matrix of shape(n_samples, n_features)\n",
    "        node_indices (list):     List containing the active indices. I.e, the samples being considered at this step.\n",
    "        feature (int):           Index of feature to split on\n",
    "    \n",
    "    Returns:\n",
    "        left_indices (list):     Indices with feature value == 1\n",
    "        right_indices (list):    Indices with feature value == 0\n",
    "    \"\"\"\n",
    "    \n",
    "    # You need to return the following variables correctly\n",
    "    left_indices = []\n",
    "    right_indices = []\n",
    "    \n",
    "    for i in node_indices:\n",
    "        if X[:i+1, feature][i] == 1:\n",
    "            left_indices.append(i)\n",
    "        else:\n",
    "            right_indices.append(i)\n",
    "        \n",
    "    return left_indices, right_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos la función y visualizamos los resultados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left indices:  [0, 1, 2, 3, 4, 7, 9]\n",
      "Right indices:  [5, 6, 8]\n"
     ]
    }
   ],
   "source": [
    "root_indices = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "feature = 0\n",
    "\n",
    "left_indices, right_indices = split_dataset(X_train, root_indices, feature)\n",
    "\n",
    "print(\"Left indices: \", left_indices)\n",
    "print(\"Right indices: \", right_indices)\n",
    "\n",
    "# Visualize the split \n",
    "#generate_split_viz(root_indices, left_indices, right_indices, feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906d281f",
   "metadata": {},
   "source": [
    "<img src=\"images/vis1.png\" alt=\"drawing\" width=\"1000\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3  Calcular ganancia de información\n",
    "\n",
    "Ahora que tenemos una función para dividir los datos en ramas izquierda y derecha, podemos calcular la ganancia de información de dividir en una característica dada.gain from the split.\n",
    "\n",
    "$$\\text{Information Gain} = H(p_1^\\text{node})- (w^{\\text{left}}H(p_1^\\text{left}) + w^{\\text{right}}H(p_1^\\text{right}))$$\n",
    "\n",
    "Dónde: \n",
    "- $H(p_1^\\text{node})$ es la entropía en el nodo antes de la división\n",
    "- $H(p_1^\\text{left})$ and $H(p_1^\\text{right})$ son las entropías en las ramas izquierda y derecha después de la división\n",
    "- $w^{\\text{left}}$ and $w^{\\text{right}}$ son las proporciones de ejemplos en las ramas izquierda y derecha, respectivamente\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_information_gain(X, y, node_indices, feature):\n",
    "    \n",
    "    \"\"\"\n",
    "    Compute the information of splitting the node on a given feature\n",
    "    \n",
    "    Args:\n",
    "        X (ndarray):            Data matrix of shape(n_samples, n_features)\n",
    "        y (array like):         list or ndarray with n_samples containing the target variable\n",
    "        node_indices (ndarray): List containing the active indices. I.e, the samples being considered in this step.\n",
    "   \n",
    "    Returns:\n",
    "        cost (float):        Cost computed\n",
    "    \n",
    "    \"\"\"    \n",
    "    # Split dataset\n",
    "    left_indices, right_indices = split_dataset(X, node_indices, feature)\n",
    "    \n",
    "    # Some useful variables\n",
    "    X_node, y_node = X[node_indices], y[node_indices]\n",
    "    X_left, y_left = X[left_indices], y[left_indices]\n",
    "    X_right, y_right = X[right_indices], y[right_indices]\n",
    "    \n",
    "    # You need to return the following variables correctly\n",
    "    information_gain = 0\n",
    "    \n",
    "    entropy_node = compute_entropy(y_node)\n",
    "    entropy_left = compute_entropy(y_left)\n",
    "    entropy_right = compute_entropy(y_right)\n",
    "    \n",
    "    w_left = len(X_left) / len(X_node)\n",
    "    w_right = len(X_right) / len(X_node)\n",
    "    \n",
    "    information_gain = entropy_node - ((w_left * entropy_left) + (w_right * entropy_right))\n",
    "    \n",
    "    return information_gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecutamos la función en el root node, probando cada una de las tres características:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information Gain resultado de dividir por color del sombrero (marrón):  0.034851554559677034\n",
      "Information Gain resultado de dividir por la forma del tallo:  0.12451124978365313\n",
      "Information Gain resultado de dividir en función de si la seta se encuentra en solitario:  0.2780719051126377\n"
     ]
    }
   ],
   "source": [
    "info_gain0 = compute_information_gain(X_train, y_train, root_indices, feature=0)\n",
    "print(\"Information Gain resultado de dividir por color del sombrero (marrón): \", info_gain0)\n",
    "\n",
    "info_gain1 = compute_information_gain(X_train, y_train, root_indices, feature=1)\n",
    "print(\"Information Gain resultado de dividir por la forma del tallo: \", info_gain1)\n",
    "\n",
    "info_gain2 = compute_information_gain(X_train, y_train, root_indices, feature=2)\n",
    "print(\"Information Gain resultado de dividir en función de si la seta se encuentra en solitario: \", info_gain2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividir por \"solitary\" (feature = 2) en el nodo raíz da la máxima ganancia de información. Por lo tanto, es la mejor característica para comenzar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4  Get best split\n",
    "Por último, escribiremos una función para obtener la mejor característica para dividir en un nodo dado.\n",
    "- La función toma los datos en un nodo, `X`, y devuelve el índice de la característica que da la máxima ganancia de información.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_split(X, y, node_indices):   \n",
    "    \"\"\"\n",
    "    Returns the optimal feature and threshold value\n",
    "    to split the node data \n",
    "    \n",
    "    Args:\n",
    "        X (ndarray):            Data matrix of shape(n_samples, n_features)\n",
    "        y (array like):         list or ndarray with n_samples containing the target variable\n",
    "        node_indices (ndarray): List containing the active indices. I.e, the samples being considered in this step.\n",
    "\n",
    "    Returns:\n",
    "        best_feature (int):     The index of the best feature to split\n",
    "    \"\"\"    \n",
    "    \n",
    "    # Some useful variables\n",
    "    num_features = X.shape[1]\n",
    "    \n",
    "    # You need to return the following variables correctly\n",
    "    best_feature = -1\n",
    "    max_info_gain = 0\n",
    "    \n",
    "    for feature in range(num_features):\n",
    "        info_gain = compute_information_gain(X, y, node_indices, feature=feature)\n",
    "        \n",
    "        if info_gain > max_info_gain:\n",
    "            max_info_gain = info_gain\n",
    "            best_feature = feature\n",
    "   \n",
    "    return best_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecutamos la función:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best feature to split on: 2\n"
     ]
    }
   ],
   "source": [
    "best_feature = get_best_split(X_train, y_train, root_indices)\n",
    "print(\"Best feature to split on: %d\" % best_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"5\"></a>\n",
    "## 5 - Generar el árbol de decisión\n",
    "\n",
    "Utilizamos las funciones anteriores para generar un árbol de decisión, estableciendo una profundidad máxima de 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = []\n",
    "\n",
    "def build_tree_recursive(X, y, node_indices, branch_name, max_depth, current_depth):\n",
    "    \"\"\"\n",
    "    Build a tree using the recursive algorithm that split the dataset into 2 subgroups at each node.\n",
    "    This function just prints the tree.\n",
    "    \n",
    "    Args:\n",
    "        X (ndarray):            Data matrix of shape(n_samples, n_features)\n",
    "        y (array like):         list or ndarray with n_samples containing the target variable\n",
    "        node_indices (ndarray): List containing the active indices. I.e, the samples being considered in this step.\n",
    "        branch_name (string):   Name of the branch. ['Root', 'Left', 'Right']\n",
    "        max_depth (int):        Max depth of the resulting tree. \n",
    "        current_depth (int):    Current depth. Parameter used during recursive call.\n",
    "   \n",
    "    \"\"\" \n",
    "\n",
    "    # Maximum depth reached - stop splitting\n",
    "    if current_depth == max_depth:\n",
    "        formatting = \" \"*current_depth + \"-\"*current_depth\n",
    "        print(formatting, \"%s leaf node with indices\" % branch_name, node_indices)\n",
    "        return\n",
    "   \n",
    "    # Otherwise, get best split and split the data\n",
    "    # Get the best feature and threshold at this node\n",
    "    best_feature = get_best_split(X, y, node_indices) \n",
    "    \n",
    "    formatting = \"-\"*current_depth\n",
    "    print(\"%s Depth %d, %s: Split on feature: %d\" % (formatting, current_depth, branch_name, best_feature))\n",
    "    \n",
    "    # Split the dataset at the best feature\n",
    "    left_indices, right_indices = split_dataset(X, node_indices, best_feature)\n",
    "    tree.append((left_indices, right_indices, best_feature))\n",
    "    \n",
    "    # continue splitting the left and the right child. Increment current depth\n",
    "    build_tree_recursive(X, y, left_indices, \"Left\", max_depth, current_depth+1)\n",
    "    build_tree_recursive(X, y, right_indices, \"Right\", max_depth, current_depth+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'build_tree_recursive' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\icira\\wCrkspace\\ceste_ml\\notebooks\\4_DecisionTrees\\4_DecisionTrees.ipynb Cell 28\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/icira/wCrkspace/ceste_ml/notebooks/4_DecisionTrees/4_DecisionTrees.ipynb#X36sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m build_tree_recursive(X_train, y_train, root_indices, \u001b[39m\"\u001b[39m\u001b[39mRoot\u001b[39m\u001b[39m\"\u001b[39m, max_depth\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, current_depth\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/icira/wCrkspace/ceste_ml/notebooks/4_DecisionTrees/4_DecisionTrees.ipynb#X36sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m#generate_tree_viz(root_indices, y_train, tree)\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'build_tree_recursive' is not defined"
     ]
    }
   ],
   "source": [
    "build_tree_recursive(X_train, y_train, root_indices, \"Root\", max_depth=3, current_depth=0)\n",
    "#generate_tree_viz(root_indices, y_train, tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981c46a7",
   "metadata": {},
   "source": [
    "<img src=\"images/vis2.png\" alt=\"drawing\" width=\"1000\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1b8918",
   "metadata": {},
   "source": [
    "# Decision Trees: Sampling With Replacement\n",
    "\n",
    "Vamos a repetir el proceso anterior, pero esta vez, en lugar de dividir en la característica que da la máxima ganancia de información, elegiremos una característica al azar de las tres características posibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6b2e0e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate bootstrap samples\n",
    "def generate_bootstrap_samples(X_train, y_train, B):\n",
    "    \"\"\"\n",
    "    Generate B bootstrapped samples from the training data\n",
    "    \n",
    "    Args:\n",
    "        X_train (ndarray): Data matrix of shape(n_samples, n_features)\n",
    "        y_train (array like): list or ndarray with n_samples containing the target variable\n",
    "        B (int): Number of bootstrapped samples to generate\n",
    "    \n",
    "    Returns:\n",
    "        X_bootstrap (ndarray): Data matrix of shape(B, n_samples, n_features)\n",
    "        y_bootstrap (ndarray): Data matrix of shape(B, n_samples)\n",
    "    \"\"\"\n",
    "    \n",
    "    # You need to return the following variables correctly\n",
    "    X_bootstrap = []\n",
    "    y_bootstrap = []\n",
    "    \n",
    "    for i in range(B):\n",
    "        indices = np.random.choice(range(len(X_train)), len(X_train), replace=True)\n",
    "        X_bootstrap.append(X_train[indices])\n",
    "        y_bootstrap.append(y_train[indices])\n",
    "    \n",
    "    return np.array(X_bootstrap), np.array(y_bootstrap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dff8c05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped X shape:  (3, 10, 3)\n",
      "Bootstrapped y shape:  (3, 10)\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Bootstrapped X for tree 0:\n",
      " [[1 0 1]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 1 1]\n",
      " [1 1 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 1]\n",
      " [1 0 1]]\n",
      "Bootstrapped y for tree 0:\n",
      " [1 1 0 1 1 1 0 0 1 1]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Bootstrapped X for tree 1:\n",
      " [[1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 1 1]\n",
      " [1 1 1]\n",
      " [0 0 0]\n",
      " [1 0 1]\n",
      " [1 1 1]\n",
      " [1 0 0]]\n",
      "Bootstrapped y for tree 1:\n",
      " [0 0 1 0 1 1 0 1 1 0]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Bootstrapped X for tree 2:\n",
      " [[1 0 0]\n",
      " [1 1 1]\n",
      " [0 1 0]\n",
      " [1 1 1]\n",
      " [0 0 0]\n",
      " [1 0 0]\n",
      " [1 0 1]\n",
      " [1 0 1]\n",
      " [1 1 1]\n",
      " [0 0 0]]\n",
      "Bootstrapped y for tree 2:\n",
      " [0 1 1 1 0 0 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "# Num trees B = 3\n",
    "B = 3\n",
    "\n",
    "# Generate bootstrap samples\n",
    "bootstrapped_X, bootstrapped_y = generate_bootstrap_samples(X_train, y_train, B)\n",
    "\n",
    "print(\"Bootstrapped X shape: \", bootstrapped_X.shape)\n",
    "print(\"Bootstrapped y shape: \", bootstrapped_y.shape)\n",
    "\n",
    "for i in range(B):\n",
    "    print(\"\\n\" + \"-\"*100 + \"\\n\")\n",
    "    print(\"Bootstrapped X for tree %d:\\n\" % i, bootstrapped_X[i])\n",
    "    print(\"Bootstrapped y for tree %d:\\n\" % i, bootstrapped_y[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b98c2d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Depth 0, Root: Split on feature: 2\n",
      "- Depth 1, Left: Split on feature: 0\n",
      "  -- Left leaf node with indices [0, 4, 5, 8, 9]\n",
      "  -- Right leaf node with indices []\n",
      "- Depth 1, Right: Split on feature: 0\n",
      "  -- Left leaf node with indices [2, 6, 7]\n",
      "  -- Right leaf node with indices [1, 3]\n",
      "\n",
      " Depth 0, Root: Split on feature: 1\n",
      "- Depth 1, Left: Split on feature: 0\n",
      "  -- Left leaf node with indices [4, 5, 8]\n",
      "  -- Right leaf node with indices [2]\n",
      "- Depth 1, Right: Split on feature: 2\n",
      "  -- Left leaf node with indices [7]\n",
      "  -- Right leaf node with indices [0, 1, 3, 6, 9]\n",
      "\n",
      " Depth 0, Root: Split on feature: 2\n",
      "- Depth 1, Left: Split on feature: 0\n",
      "  -- Left leaf node with indices [1, 3, 6, 7, 8]\n",
      "  -- Right leaf node with indices []\n",
      "- Depth 1, Right: Split on feature: 1\n",
      "  -- Left leaf node with indices [2]\n",
      "  -- Right leaf node with indices [0, 4, 5, 9]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate a tree for each bootstrapped sample\n",
    "trees = []\n",
    "for i in range(B):\n",
    "    tree = []\n",
    "    root_indices = range(len(bootstrapped_X[i]))\n",
    "    build_tree_recursive(bootstrapped_X[i], bootstrapped_y[i], root_indices, \"Root\", max_depth=2, current_depth=0)\n",
    "    trees.append(tree)\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "32d459b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict a new sample\n",
    "def predict(sample, tree):\n",
    "    \"\"\"\n",
    "    Predicts the label for a single sample by \n",
    "    recursively descending the tree until a leaf node is hit\n",
    "    \n",
    "    Args:\n",
    "        sample (ndarray): Data matrix of shape(n_features)\n",
    "        tree (list):      List of tuples containing the tree information\n",
    "    \n",
    "    Returns:\n",
    "        label (int):      Label predicted for the sample\n",
    "    \"\"\"\n",
    "    \n",
    "    # You need to return the following variables correctly\n",
    "    label = None\n",
    "    \n",
    "    # Descend the tree until a leaf node is hit\n",
    "    for node in tree:\n",
    "        left_indices, right_indices, feature = node\n",
    "        \n",
    "        if sample[feature] == 1:\n",
    "            if len(left_indices) == 0:\n",
    "                label = 0\n",
    "                break\n",
    "            else:\n",
    "                node = left_indices\n",
    "        else:\n",
    "            if len(right_indices) == 0:\n",
    "                label = 1\n",
    "                break\n",
    "            else:\n",
    "                node = right_indices\n",
    "    \n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aba56960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Majority vote\n",
    "def majority_vote(predictions):\n",
    "    \"\"\"\n",
    "    Returns the majority label from a list of predictions\n",
    "    \n",
    "    Args:\n",
    "        predictions (list): List of predictions\n",
    "    \n",
    "    Returns:\n",
    "        label (int):        Majority label\n",
    "    \"\"\"\n",
    "    \n",
    "    # You need to return the following variables correctly\n",
    "    label = None\n",
    "    \n",
    "    for i in range(len(predictions)):\n",
    "        if predictions[i] == None:\n",
    "            predictions[i] = 0\n",
    "            \n",
    "    p1 = len(predictions[predictions==1])\n",
    "    p0 = len(predictions[predictions==0])\n",
    "    \n",
    "    if p1 > p0:\n",
    "        label = 1\n",
    "    else:\n",
    "        label = 0\n",
    "    \n",
    "    return label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b164e43",
   "metadata": {},
   "source": [
    "# XGBoost\n",
    "\n",
    "A continuación resolveremos el mismo ejemplo con XGBoost, para comparar los resultados.\n",
    "\n",
    "### 6.1  Importar paquetes\n",
    "\n",
    "Será necesario haber instalado librerías adicionales:\n",
    "\n",
    "- `pip install xgboost`\n",
    "- `pip install scikit-learn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "608d2c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar paquetes para XGBoost\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fec2851",
   "metadata": {},
   "source": [
    "Vamos a dividir los datos en dos subconjuntos: entrenamiento y test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "741a13f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a21d909a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1]\n",
      " [1 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 1 1]\n",
      " [0 1 1]\n",
      " [0 0 0]\n",
      " [1 0 1]\n",
      " [0 1 0]\n",
      " [1 0 0]]\n",
      "[[0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)\n",
    "print(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76826bf",
   "metadata": {},
   "source": [
    "Transformamos los datos a formato `DMatrix` de XGBoost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "eda2eaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d320a6",
   "metadata": {},
   "source": [
    "Establecemos los parámetros de trabajo:\n",
    " - Profundidad máxima del árbol: `max_depth`\n",
    " - Tasa de aprendizaje: `eta`\n",
    " - Objetivo: `objective`\n",
    " - Número de árboles: `num_round`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "51ead985",
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    'max_depth': 2,\n",
    "    'eta': 1,\n",
    "    'objective': 'binary:logistic'\n",
    "}\n",
    "num_round = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998277f7",
   "metadata": {},
   "source": [
    "Entrenamos/construimos el árbol:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d7f6f334",
   "metadata": {},
   "outputs": [],
   "source": [
    "bst = xgb.train(param, dtrain, num_round)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7495ef",
   "metadata": {},
   "source": [
    "Y lo probamos con el ejemplo que hemos separado para test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "66bb7536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "preds = bst.predict(dtest)\n",
    "predictions = [round(value) for value in preds]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a522cc11",
   "metadata": {},
   "source": [
    "Para por último evaluar la precisión del modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2f7c03c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.00%\n",
      "\n",
      "X: [0 1 0] - y: [1] --> y_pred: 0\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print()\n",
    "for i, x in enumerate(X_test):\n",
    "    print(f\"X: {x} - y: [{y_test[i]}] --> y_pred: {predictions[i]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
